NKP Cluster Storage Troubleshooting Guide
Environment Variables
bashCONTROL_PLANES="10.38.235.41 10.38.235.80 10.38.235.71"
WORKERS="10.38.235.56 10.38.235.75 10.38.235.66 10.38.235.90"
ALL_NODES="$CONTROL_PLANES $WORKERS"
CONTROL_PLANE_VIP="10.38.235.110"
VIP_INTERFACE="ens3"
CLUSTER_NAME="nkp-poc"

1. Check Current Storage Status
Check mounted volumes on all workers
bashfor ip in $WORKERS; do
    echo "=== Worker $ip ==="
    ssh nutanix@$ip "df -h /mnt/disks/* 2>/dev/null | grep -v Filesystem"
    echo ""
done
Check what's actually mounted in /proc/mounts
bashfor ip in $WORKERS; do
    echo "=== Worker $ip mounts ==="
    ssh nutanix@$ip "mount | grep /mnt/disks"
    echo ""
done
Check local-volume-provisioner logs
bash# Bootstrap cluster
kubectl logs -n kube-system daemonset/local-volume-provisioner --tail=50

# Workload cluster
kubectl --kubeconfig=nkp-poc.conf logs -n kube-system daemonset/local-volume-provisioner --tail=50
Check PVs and PVCs
bash# Bootstrap cluster
kubectl get pv -o wide
kubectl get pvc -A

# Workload cluster  
kubectl --kubeconfig=nkp-poc.conf get pv -o wide
kubectl --kubeconfig=nkp-poc.conf get pvc -A

2. Fix Storage Issues
Create missing volumes (Quick fix with tmpfs)
bash# On each worker - create vol5-20 as tmpfs
for ip in $WORKERS; do
    echo "=== Setting up volumes on $ip ==="
    ssh nutanix@$ip '
        for i in {5..20}; do
            sudo mkdir -p /mnt/disks/vol${i}
            sudo mount -t tmpfs -o size=1G tmpfs /mnt/disks/vol${i}
        done
        echo "Mounted volumes:"
        mount | grep /mnt/disks | wc -l
    '
done
Create proper loop device volumes (Production approach)
bash# On each worker - create 100GB sparse files with loop devices
for ip in $WORKERS; do
    echo "=== Setting up loop volumes on $ip ==="
    ssh nutanix@$ip '
        sudo mkdir -p /var/lib/k8s-volumes
        
        # Create volumes 1-4 with 100GB each
        for i in {1..4}; do
            if [ ! -f /var/lib/k8s-volumes/vol${i}.img ]; then
                sudo truncate -s 100G /var/lib/k8s-volumes/vol${i}.img
                sudo mkfs.ext4 -q /var/lib/k8s-volumes/vol${i}.img
            fi
            sudo mkdir -p /mnt/disks/vol${i}
            sudo mount -o loop /var/lib/k8s-volumes/vol${i}.img /mnt/disks/vol${i} 2>/dev/null || true
        done
        
        # Add to fstab for persistence
        for i in {1..4}; do
            grep -q "vol${i}.img" /etc/fstab || \
            echo "/var/lib/k8s-volumes/vol${i}.img /mnt/disks/vol${i} ext4 loop,defaults 0 0" | sudo tee -a /etc/fstab
        done
        
        # Verify
        df -h /mnt/disks/vol* | grep -v Filesystem
    '
done
Create helm-repository directories
bashfor ip in $WORKERS; do
    echo "=== Creating helm directories on $ip ==="
    ssh nutanix@$ip '
        sudo mkdir -p /tmp/helm-charts /var/tmp/helm-repo
        sudo chmod 777 /tmp/helm-charts /var/tmp/helm-repo
        ls -ld /tmp/helm-charts /var/tmp/helm-repo
    '
done

3. Fix Helm Repository PVC Issues
Create manual PV for helm-repository (Bootstrap cluster)
bashcat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: helm-repo-pv-manual
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: localvolumeprovisioner
  hostPath:
    path: /tmp/helm-charts
    type: DirectoryOrCreate
EOF

# Check if PVC binds
kubectl get pvc -n caren-system
Create manual PV for helm-repository (Workload cluster)
bashcat <<EOF | kubectl --kubeconfig=nkp-poc.conf apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: helm-repo-pv-manual
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: localvolumeprovisioner
  hostPath:
    path: /tmp/helm-charts
    type: DirectoryOrCreate
EOF

# Check if PVC binds
kubectl --kubeconfig=nkp-poc.conf get pvc -n caren-system
Force PVC recreation if stuck
bash# Bootstrap cluster
kubectl delete pvc helm-charts-pvc -n caren-system --force --grace-period=0
kubectl patch pvc helm-charts-pvc -n caren-system -p '{"metadata":{"finalizers":null}}'

# Workload cluster
kubectl --kubeconfig=nkp-poc.conf delete pvc helm-charts-pvc -n caren-system --force --grace-period=0
kubectl --kubeconfig=nkp-poc.conf patch pvc helm-charts-pvc -n caren-system -p '{"metadata":{"finalizers":null}}'

4. Restart Components
Restart local-volume-provisioner
bash# Bootstrap cluster
kubectl rollout restart daemonset/local-volume-provisioner -n kube-system
kubectl rollout status daemonset/local-volume-provisioner -n kube-system

# Workload cluster
kubectl --kubeconfig=nkp-poc.conf rollout restart daemonset/local-volume-provisioner -n kube-system
kubectl --kubeconfig=nkp-poc.conf rollout status daemonset/local-volume-provisioner -n kube-system
Restart helm-repository if needed
bash# Bootstrap cluster
kubectl delete pod -n caren-system -l app.kubernetes.io/name=helm-repository

# Workload cluster
kubectl --kubeconfig=nkp-poc.conf delete pod -n caren-system -l app.kuber
